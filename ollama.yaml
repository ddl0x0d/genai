# https://github.com/ollama/ollama/blob/main/docs/api.md
openapi: 3.1.0

info:
  title: Ollama REST API
  description: |
    API for interacting with locally hosted large language models (LLMs) using Ollama.

    Based on [Ollama REST API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md).
  version: 0.3.13

servers:
  - url: http://localhost:11434
    description: Local Ollama server

tags:
  - name: Completion
  - name: Model
  - name: Embeddings

paths:
  /api/generate:
    post:
      summary: Generate a completion
      operationId: generateCompletion
      tags: [ Completion ]
      description: |
        Generate a response for a given prompt with a provided model.
        This is a streaming endpoint, so there will be a series of responses.
        The final response object will include statistics and additional data from the request.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion
      requestBody:
        $ref: "#/components/requestBodies/GenerateCompletionRequest"
      responses:
        200:
          $ref: "#/components/responses/GenerateCompletionResponse"

  /api/chat:
    post:
      summary: Generate a chat completion
      operationId: generateChatCompletion
      tags: [ Completion ]
      description: |
        Generate the next message in a chat with a provided model.
        This is a streaming endpoint, so there will be a series of responses.
        Streaming can be disabled using `"stream": false`.
        The final response object will include statistics and additional data from the request.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion
      requestBody:
        $ref: "#/components/requestBodies/GenerateChatCompletionRequest"
      responses:
        200:
          $ref: "#/components/responses/GenerateChatCompletionResponse"

  /api/create:
    post:
      summary: Create a Model
      operationId: createModel
      tags: [ Model ]
      description: |
        Create a model from:

        - another model;
        - a safetensors directory; or
        - a GGUF file.

        If you are creating a model from a safetensors directory or from a GGUF file, you must
        [create a blob](https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-blob)
        for each of the files and then use the file name and SHA256
        digest associated with each blob in the files field.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model
      requestBody:
        $ref: "#/components/requestBodies/CreateModelRequest"
      responses:
        200:
          $ref: "#/components/responses/CreateModelResponse"

  /api/tags:
    get:
      summary: List Local Models
      operationId: listLocalModels
      tags: [ Model ]
      description: |
        List models that are available locally.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models
      responses:
        200:
          $ref: "#/components/responses/ListLocalModelsResponse"

  /api/ps:
    get:
      summary: List Running Models
      operationId: listRunningModels
      tags: [ Model ]
      description: |
        List models that are currently loaded into memory.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models

  /api/show:
    get:
      summary: Show Model Information
      operationId: showModelInformation
      tags: [ Model ]
      description: |
        Show information about a model including details,
        modelfile, template, parameters, license, system prompt.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information
      requestBody:
        $ref: "#/components/requestBodies/ShowModelInformationRequest"
      responses:
        200:
          $ref: "#/components/responses/ShowModelInformationResponse"

  /api/copy:
    post:
      summary: Copy a Model
      operationId: copyModel
      tags: [ Model ]
      description: |
        Copy a model. Creates a model with another name from an existing model.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#copy-a-model

  /api/delete:
    delete:
      summary: Delete a Model
      operationId: deleteModel
      tags: [ Model ]
      description: |
        Delete a model and its data.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#delete-a-model

  /api/pull:
    post:
      summary: Pull a Model
      operationId: pullModel
      tags: [ Model ]
      description: |
        Download a model from the ollama library. Cancelled pulls are resumed from
        where they left off, and multiple calls will share the same download progress.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#pull-a-model

  /api/push:
    post:
      summary: Push a Model
      operationId: pushModel
      tags: [ Model ]
      description: |
        Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#push-a-model

  /api/blobs/:digest:
    head:
      summary: Check if a Blob Exists
      operationId: checkBlobExists
      tags: [ Blob ]
      description: |
        Ensures that the file blob (Binary Large Object)
        used with create a model exists on the server.
        This checks your Ollama server and not https://ollama.com.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#check-if-a-blob-exists
      parameters:
        - name: digest
          in: query
          description: The SHA256 digest of the blob.
          required: true
          schema:
            type: string

    post:
      summary: Push a Blob
      operationId: pushBlob
      tags: [ Blob ]
      description: |
        Push a file to the Ollama server to create a "blob" (Binary Large Object).
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#push-a-blob
      parameters:
        - name: digest
          in: query
          description: The SHA256 digest of the blob.
          required: true
          schema:
            type: string

  /api/embed:
    post:
      summary: Generate Embeddings
      operationId: generateEmbeddings
      tags: [ Embeddings ]
      description: |
        Generate embeddings from a model.
      externalDocs:
        url: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings
      requestBody:
        $ref: "#/components/requestBodies/GenerateEmbeddingsRequest"
      responses:
        200:
          $ref: "#/components/responses/GenerateEmbeddingsResponse"

components:

  examples:

    CreateModelRequest:
      summary: Create a new model
      description: Create a new model from a `Modelfile`.
      value: |
        {
          "name": "mario",
          "modelfile": "FROM llama3\nSYSTEM You are mario from Super Mario Bros."
        }

    GenerateChatCompletionRequestStreaming:
      summary: Streaming
      value: |
        {
          "model": "llama3.2",
          "messages": [
            {
              "role": "user",
              "content": "why is the sky blue?"
            }
          ]
        }

    GenerateChatCompletionRequestNoStreaming:
      summary: No streaming
      value: |
        {
          "model": "llama3.2",
          "messages": [
            {
              "role": "user",
              "content": "why is the sky blue?"
            }
          ],
          "stream": false
        }

    GenerateChatCompletionResponseNoStreaming:
      summary: No streaming
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-12-12T14:13:43.416799Z",
          "message": {
            "role": "assistant",
            "content": "Hello! How are you today?"
          },
          "done": true,
          "total_duration": 5191566416,
          "load_duration": 2154458,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 383809000,
          "eval_count": 298,
          "eval_duration": 4799921000
        }

    GenerateChatCompletionRequestStructuredOutputs:
      summary: Structured outputs
      value: |
        {
          "model": "llama3.1",
          "messages": [
            {
              "role": "user",
              "content": "Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability."
            }
          ],
          "stream": false,
          "format": {
            "type": "object",
            "properties": {
              "age": {
                "type": "integer"
              },
              "available": {
                "type": "boolean"
              }
            },
            "required": [
              "age",
              "available"
            ]
          },
          "options": {
            "temperature": 0
          }
        }

    GenerateChatCompletionResponseStructuredOutputs:
      summary: Structured outputs
      value: |
        {
          "model": "llama3.1",
          "created_at": "2024-12-06T00:46:58.265747Z",
          "message": { "role": "assistant", "content": "{\"age\": 22, \"available\": false}" },
          "done_reason": "stop",
          "done": true,
          "total_duration": 2254970291,
          "load_duration": 574751416,
          "prompt_eval_count": 34,
          "prompt_eval_duration": 1502000000,
          "eval_count": 12,
          "eval_duration": 175000000
        }

    GenerateChatCompletionRequestWithHistory:
      summary: With History
      description: |
        Send a chat message with a conversation history.
        You can use this same approach to start the conversation
        using multi-shot or chain-of-thought prompting.
      value: |
        {
          "model": "llama3.2",
          "messages": [
            {
              "role": "user",
              "content": "why is the sky blue?"
            },
            {
              "role": "assistant",
              "content": "due to rayleigh scattering."
            },
            {
              "role": "user",
              "content": "how is that different than mie scattering?"
            }
          ]
        }

    GenerateChatCompletionRequestWithImages:
      summary: With images
      description: |
        Send a chat message with images.
        The images should be provided as an array,
        with the individual images encoded in Base64.
      value: |
        {
          "model": "llava",
          "messages": [
            {
              "role": "user",
              "content": "what is in this image?",
              "images": ["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"]
            }
          ]
        }

    GenerateChatCompletionResponseWithImages:
      summary: With images
      value: |
        {
          "model": "llava",
          "created_at": "2023-12-13T22:42:50.203334Z",
          "message": {
            "role": "assistant",
            "content": " The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.",
            "images": null
          },
          "done": true,
          "total_duration": 1668506709,
          "load_duration": 1986209,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 359682000,
          "eval_count": 83,
          "eval_duration": 1303285000
        }

    GenerateChatCompletionRequestReproducibleOutputs:
      summary: Reproducible outputs
      value: |
        {
          "model": "llama3.2",
          "messages": [
            {
              "role": "user",
              "content": "Hello!"
            }
          ],
          "options": {
            "seed": 101,
            "temperature": 0
          }
        }

    GenerateChatCompletionResponseReproducibleOutputs:
      summary: Reproducible outputs
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-12-12T14:13:43.416799Z",
          "message": {
            "role": "assistant",
            "content": "Hello! How are you today?"
          },
          "done": true,
          "total_duration": 5191566416,
          "load_duration": 2154458,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 383809000,
          "eval_count": 298,
          "eval_duration": 4799921000
        }

    GenerateChatCompletionRequestWithTools:
      summary: With tools
      value: |
        {
          "model": "llama3.2",
          "messages": [
            {
              "role": "user",
              "content": "What is the weather today in Paris?"
            }
          ],
          "stream": false,
          "tools": [
            {
              "type": "function",
              "function": {
                "name": "get_current_weather",
                "description": "Get the current weather for a location",
                "parameters": {
                  "type": "object",
                  "properties": {
                    "location": {
                      "type": "string",
                      "description": "The location to get the weather for, e.g. San Francisco, CA"
                    },
                    "format": {
                      "type": "string",
                      "description": "The format to return the weather in, e.g. 'celsius' or 'fahrenheit'",
                      "enum": ["celsius", "fahrenheit"]
                    }
                  },
                  "required": ["location", "format"]
                }
              }
            }
          ]
        }

    GenerateChatCompletionResponseWithTools:
      summary: With tools
      value: |
        {
          "model": "llama3.2",
          "created_at": "2024-07-22T20:33:28.123648Z",
          "message": {
            "role": "assistant",
            "content": "",
            "tool_calls": [
              {
                "function": {
                  "name": "get_current_weather",
                  "arguments": {
                    "format": "celsius",
                    "location": "Paris, FR"
                  }
                }
              }
            ]
          },
          "done_reason": "stop",
          "done": true,
          "total_duration": 885095291,
          "load_duration": 3753500,
          "prompt_eval_count": 122,
          "prompt_eval_duration": 328493000,
          "eval_count": 33,
          "eval_duration": 552222000
        }

    GenerateChatCompletionRequestLoadModel:
      summary: Load a model
      description: If the messages array is empty, the model will be loaded into memory.
      value: |
        {
          "model": "llama3.2",
          "messages": []
        }

    GenerateChatCompletionResponseLoadModel:
      summary: Load a model
      value: |
        {
          "model": "llama3.2",
          "created_at":"2024-09-12T21:17:29.110811Z",
          "message": {
            "role": "assistant",
            "content": ""
          },
          "done_reason": "load",
          "done": true
        }

    GenerateChatCompletionRequestUnloadModel:
      summary: Unload a model
      description: |
        If the messages array is empty and the `keep_alive` parameter
        is set to `0`, a model will be unloaded from memory.
      value: |
        {
          "model": "llama3.2",
          "messages": [],
          "keep_alive": 0
        }

    GenerateChatCompletionResponseUnloadModel:
      summary: Unload a model
      value: |
        {
          "model": "llama3.2",
          "created_at":"2024-09-12T21:33:17.547535Z",
          "message": {
            "role": "assistant",
            "content": ""
          },
          "done_reason": "unload",
          "done": true
        }

    GenerateChatCompletionResponseStreaming:
      summary: Streaming
      description: A stream of JSON objects is returned.
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-08-04T08:52:19.385406455-07:00",
          "message": {
            "role": "assistant",
            "content": "The",
            "images": null
          },
          "done": false
        }

    GenerateChatCompletionResponseStreamingFinal:
      summary: Streaming
      description: Final response.
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-08-04T19:22:45.499127Z",
          "message": {
            "role": "assistant",
            "content": ""
          },
          "done": true,
          "total_duration": 4883583458,
          "load_duration": 1334875,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 342546000,
          "eval_count": 282,
          "eval_duration": 4535599000
        }

    GenerateCompletionRequestStreaming:
      summary: Streaming
      value: |
        {
          "model": "llama3.2",
          "prompt": "Why is the sky blue?"
        }

    GenerateCompletionResponseStreaming:
      summary: Streaming
      description: A stream of JSON objects is returned.
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-08-04T08:52:19.385406455-07:00",
          "response": "The",
          "done": false
        }

    GenerateCompletionResponseStreamingFinal:
      summary: Streaming final response
      description: The final response in the stream also includes additional data about the generation.
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-08-04T19:22:45.499127Z",
          "response": "",
          "done": true,
          "context": [1, 2, 3],
          "total_duration": 10706818083,
          "load_duration": 6338219291,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 130079000,
          "eval_count": 259,
          "eval_duration": 4232710000
        }

    GenerateCompletionRequestNoStreaming:
      summary: No streaming
      description: A response can be received in one reply when streaming is off.
      value: |
        {
          "model": "llama3.2",
          "prompt": "Why is the sky blue?",
          "stream": false
        }

    GenerateCompletionResponseNoStreaming:
      summary: No streaming
      description: If stream is set to `false`, the response will be a single JSON object.
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-08-04T19:22:45.499127Z",
          "response": "The sky is blue because it is the color of the sky.",
          "done": true,
          "context": [1, 2, 3],
          "total_duration": 5043500667,
          "load_duration": 5025959,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 325953000,
          "eval_count": 290,
          "eval_duration": 4709213000
        }

    GenerateCompletionRequestWithSuffix:
      summary: With suffix
      value: |
        {
          "model": "codellama:code",
          "prompt": "def compute_gcd(a, b):",
          "suffix": "    return result",
          "options": {
            "temperature": 0
          },
          "stream": false
        }

    GenerateCompletionResponseWithSuffix:
      summary: With suffix
      value: |
        {
          "model": "codellama:code",
          "created_at": "2024-07-22T20:47:51.147561Z",
          "response": "\n  if a == 0:\n    return b\n  else:\n    return compute_gcd(b % a, a)\n\ndef compute_lcm(a, b):\n  result = (a * b) / compute_gcd(a, b)\n",
          "done": true,
          "done_reason": "stop",
          "context": [...],
          "total_duration": 1162761250,
          "load_duration": 6683708,
          "prompt_eval_count": 17,
          "prompt_eval_duration": 201222000,
          "eval_count": 63,
          "eval_duration": 953997000
        }

    GenerateCompletionRequestStructuredOutputs:
      summary: Structured outputs
      value: |
        {
          "model": "llama3.1:8b",
          "prompt": "Ollama is 22 years old and is busy saving the world. Respond using JSON",
          "stream": false,
          "format": {
            "type": "object",
            "properties": {
              "age": {
                "type": "integer"
              },
              "available": {
                "type": "boolean"
              }
            },
            "required": [
              "age",
              "available"
            ]
          }
        }

    GenerateCompletionResponseStructuredOutputs:
      summary: Structured outputs
      value: |
        {
          "model": "llama3.1:8b",
          "created_at": "2024-12-06T00:48:09.983619Z",
          "response": "{\n  \"age\": 22,\n  \"available\": true\n}",
          "done": true,
          "done_reason": "stop",
          "context": [1, 2, 3],
          "total_duration": 1075509083,
          "load_duration": 567678166,
          "prompt_eval_count": 28,
          "prompt_eval_duration": 236000000,
          "eval_count": 16,
          "eval_duration": 269000000
        }

    GenerateCompletionRequestJSONMode:
      summary: JSON Mode
      description: |
        When format is set to json, the output will always be a well-formed JSON object.
        It's important to also instruct the model to respond in JSON.
      value: |
        {
          "model": "llama3.2",
          "prompt": "What color is the sky at different times of the day? Respond using JSON",
          "format": "json",
          "stream": false
        }

    GenerateCompletionResponseJSONMode:
      summary: JSON Mode
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-11-09T21:07:55.186497Z",
          "response": "{\n\"morning\": {\n\"color\": \"blue\"\n},\n\"noon\": {\n\"color\": \"blue-gray\"\n},\n\"afternoon\": {\n\"color\": \"warm gray\"\n},\n\"evening\": {\n\"color\": \"orange\"\n}\n}\n",
          "done": true,
          "context": [1, 2, 3],
          "total_duration": 4648158584,
          "load_duration": 4071084,
          "prompt_eval_count": 36,
          "prompt_eval_duration": 439038000,
          "eval_count": 180,
          "eval_duration": 4196918000
        }

    GenerateCompletionRequestWithImages:
      summary: With images
      description: |
        To submit images to multimodal models such as `llava`
        or `bakllava`, provide a list of base64-encoded images.
      value: |
        {
          "model": "llava",
          "prompt":"What is in this picture?",
          "stream": false,
          "images": ["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"]
        }

    GenerateCompletionResponseWithImages:
      summary: With images
      value: |
        {
          "model": "llava",
          "created_at": "2023-11-03T15:36:02.583064Z",
          "response": "A happy cartoon character, which is cute and cheerful.",
          "done": true,
          "context": [1, 2, 3],
          "total_duration": 2938432250,
          "load_duration": 2559292,
          "prompt_eval_count": 1,
          "prompt_eval_duration": 2195557000,
          "eval_count": 44,
          "eval_duration": 736432000
        }

    GenerateCompletionRequestRawMode:
      summary: Raw Mode
      description: |
        In some cases, you may wish to bypass the templating system and provide a full prompt.
        In this case, you can use the `raw` parameter to disable templating.
        Also note that raw mode will not return a context.
      value: |
        {
          "model": "mistral",
          "prompt": "[INST] why is the sky blue? [/INST]",
          "raw": true,
          "stream": false
        }

    GenerateCompletionRequestReproducibleOutputs:
      summary: Reproducible outputs
      description: For reproducible outputs, set `seed` to a number.
      value: |
        {
          "model": "mistral",
          "prompt": "Why is the sky blue?",
          "options": {
            "seed": 123
          }
        }

    GenerateCompletionResponseReproducibleOutputs:
      summary: Reproducible outputs
      value: |
        {
          "model": "mistral",
          "created_at": "2023-11-03T15:36:02.583064Z",
          "response": " The sky appears blue because of a phenomenon called Rayleigh scattering.",
          "done": true,
          "total_duration": 8493852375,
          "load_duration": 6589624375,
          "prompt_eval_count": 14,
          "prompt_eval_duration": 119039000,
          "eval_count": 110,
          "eval_duration": 1779061000
        }

    GenerateCompletionRequestWithOptions:
      summary: With options
      description: |
        If you want to set custom options for the model at runtime rather
        than in the `Modelfile`, you can do so with the `options` parameter.
        This example sets every available option, but you can set any of
        them individually and omit the ones you do not want to override.
      value: |
        {
          "model": "llama3.2",
          "prompt": "Why is the sky blue?",
          "stream": false,
          "options": {
            "num_keep": 5,
            "seed": 42,
            "num_predict": 100,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "tfs_z": 0.5,
            "typical_p": 0.7,
            "repeat_last_n": 33,
            "temperature": 0.8,
            "repeat_penalty": 1.2,
            "presence_penalty": 1.5,
            "frequency_penalty": 1.0,
            "mirostat": 1,
            "mirostat_tau": 0.8,
            "mirostat_eta": 0.6,
            "penalize_newline": true,
            "stop": ["\n", "user:"],
            "numa": false,
            "num_ctx": 1024,
            "num_batch": 2,
            "num_gpu": 1,
            "main_gpu": 0,
            "low_vram": false,
            "f16_kv": true,
            "vocab_only": false,
            "use_mmap": true,
            "use_mlock": false,
            "num_thread": 8
          }
        }

    GenerateCompletionResponseWithOptions:
      summary: With options
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-08-04T19:22:45.499127Z",
          "response": "The sky is blue because it is the color of the sky.",
          "done": true,
          "context": [1, 2, 3],
          "total_duration": 4935886791,
          "load_duration": 534986708,
          "prompt_eval_count": 26,
          "prompt_eval_duration": 107345000,
          "eval_count": 237,
          "eval_duration": 4289432000
        }

    GenerateCompletionRequestLoadModel:
      summary: Load a model
      description: If an empty prompt is provided, the model will be loaded into memory.
      value: |
        {
          "model": "llama3.2"
        }

    GenerateCompletionResponseLoadModel:
      summary: Load a model
      value: |
        {
          "model": "llama3.2",
          "created_at": "2023-12-18T19:52:07.071755Z",
          "response": "",
          "done": true
        }

    GenerateCompletionRequestUnloadModel:
      summary: Unload a model
      description: |
        If an empty prompt is provided and the `keep_alive` parameter
        is set to `0`, a model will be unloaded from memory.
      value: |
        {
          "model": "llama3.2",
          "keep_alive": 0
        }

    GenerateCompletionResponseUnloadModel:
      summary: Unload a model
      value: |
        {
          "model": "llama3.2",
          "created_at": "2024-09-12T03:54:03.516566Z",
          "response": "",
          "done": true,
          "done_reason": "unload"
        }

    GenerateEmbeddingsRequestSingleInput:
      summary: Single input
      value: |
        {
          "model": "all-minilm",
          "input": "Why is the sky blue?"
        }

    GenerateEmbeddingsResponseSingleInput:
      summary: Single input
      value: |
        {
          "model": "all-minilm",
          "embeddings": [[
            0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
            0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
          ]],
          "total_duration": 14143917,
          "load_duration": 1019500,
          "prompt_eval_count": 8
        }

    GenerateEmbeddingsRequestMultipleInput:
      summary: Multiple input
      value: |
        {
          "model": "all-minilm",
          "input": ["Why is the sky blue?", "Why is the grass green?"]
        }

    GenerateEmbeddingsResponseMultipleInput:
      summary: Multiple input
      value: |
        {
          "model": "all-minilm",
          "embeddings": [[
            0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
            0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
          ],[
            -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,
            0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481
          ]]
        }

    ListLocalModelsResponse:
      value: |
        {
          "models": [
            {
              "name": "codellama:13b",
              "modified_at": "2023-11-04T14:56:49.277302595-07:00",
              "size": 7365960935,
              "digest": "9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697",
              "details": {
                "format": "gguf",
                "family": "llama",
                "families": null,
                "parameter_size": "13B",
                "quantization_level": "Q4_0"
              }
            },
            {
              "name": "llama3:latest",
              "modified_at": "2023-12-07T09:32:18.757212583-08:00",
              "size": 3825819519,
              "digest": "fe938a131f40e6f6d40083c9f0f430a515233eb2edaa6d72eb85c50d64f2300e",
              "details": {
                "format": "gguf",
                "family": "llama",
                "families": null,
                "parameter_size": "7B",
                "quantization_level": "Q4_0"
              }
            }
          ]
        }

  requestBodies:

    CreateModelRequest:
      required: true
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/CreateModelRequest"
          examples:
            CreateModelRequest: { $ref: "#/components/examples/CreateModelRequest" }

    GenerateChatCompletionRequest:
      required: true
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/GenerateChatCompletionRequest"
          examples:
            Streaming: { $ref: "#/components/examples/GenerateChatCompletionRequestStreaming" }
            NoStreaming: { $ref: "#/components/examples/GenerateChatCompletionRequestNoStreaming" }
            StructuredOutputs: { $ref: "#/components/examples/GenerateChatCompletionRequestStructuredOutputs" }
            WithHistory: { $ref: "#/components/examples/GenerateChatCompletionRequestWithHistory" }
            WithImages: { $ref: "#/components/examples/GenerateChatCompletionRequestWithImages" }
            ReproducibleOutputs: { $ref: "#/components/examples/GenerateChatCompletionRequestReproducibleOutputs" }
            WithTools: { $ref: "#/components/examples/GenerateChatCompletionRequestWithTools" }
            LoadModel: { $ref: "#/components/examples/GenerateChatCompletionRequestLoadModel" }
            UnloadModel: { $ref: "#/components/examples/GenerateChatCompletionRequestUnloadModel" }

    GenerateCompletionRequest:
      required: true
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/GenerateCompletionRequest"
          examples:
            Streaming: { $ref: "#/components/examples/GenerateCompletionRequestStreaming" }
            NoStreaming: { $ref: "#/components/examples/GenerateCompletionRequestNoStreaming" }
            Suffix: { $ref: "#/components/examples/GenerateCompletionRequestWithSuffix" }
            StructuredOutputs: { $ref: "#/components/examples/GenerateCompletionRequestStructuredOutputs" }
            JSONMode: { $ref: "#/components/examples/GenerateCompletionRequestJSONMode" }
            Images: { $ref: "#/components/examples/GenerateCompletionRequestWithImages" }
            RawMode: { $ref: "#/components/examples/GenerateCompletionRequestRawMode" }
            ReproducibleOutputs: { $ref: "#/components/examples/GenerateCompletionRequestReproducibleOutputs" }
            WithOptions: { $ref: "#/components/examples/GenerateCompletionRequestWithOptions" }
            LoadModel: { $ref: "#/components/examples/GenerateCompletionRequestLoadModel" }
            UnloadModel: { $ref: "#/components/examples/GenerateCompletionRequestUnloadModel" }

    GenerateEmbeddingsRequest:
      required: true
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/GenerateEmbeddingsRequest"
          examples:
            SingleInput: { $ref: "#/components/examples/GenerateEmbeddingsRequestSingleInput" }
            MultipleInput: { $ref: "#/components/examples/GenerateEmbeddingsRequestMultipleInput" }

    ShowModelInformationRequest:
      required: true
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/ShowModelInformationRequest"

  responses:

    CreateModelResponse:
      description: OK
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/CreateModelResponse"

    GenerateChatCompletionResponse:
      description: OK
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/GenerateChatCompletionResponse"
          examples:
            Streaming: { $ref: "#/components/examples/GenerateChatCompletionResponseStreaming" }
            StreamingFinal: { $ref: "#/components/examples/GenerateChatCompletionResponseStreamingFinal" }
            NoStreaming: { $ref: "#/components/examples/GenerateChatCompletionResponseNoStreaming" }
            StructuredOutputs: { $ref: "#/components/examples/GenerateChatCompletionResponseStructuredOutputs" }
            WithImages: { $ref: "#/components/examples/GenerateChatCompletionResponseWithImages" }
            ReproducibleOutputs: { $ref: "#/components/examples/GenerateChatCompletionResponseReproducibleOutputs" }
            WithTools: { $ref: "#/components/examples/GenerateChatCompletionResponseWithTools" }
            LoadModel: { $ref: "#/components/examples/GenerateChatCompletionResponseLoadModel" }
            UnloadModel: { $ref: "#/components/examples/GenerateChatCompletionResponseUnloadModel" }

    GenerateCompletionResponse:
      description: OK
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/GenerateCompletionResponse"
          examples:
            Streaming: { $ref: "#/components/examples/GenerateCompletionResponseStreaming" }
            StreamingFinal: { $ref: "#/components/examples/GenerateCompletionResponseStreamingFinal" }
            NoStreaming: { $ref: "#/components/examples/GenerateCompletionResponseNoStreaming" }
            WithSuffix: { $ref: "#/components/examples/GenerateCompletionResponseWithSuffix" }
            StructuredOutputs: { $ref: "#/components/examples/GenerateCompletionResponseStructuredOutputs" }
            JSONMode: { $ref: "#/components/examples/GenerateCompletionResponseJSONMode" }
            Images: { $ref: "#/components/examples/GenerateCompletionResponseWithImages" }
            ReproducibleOutputs: { $ref: "#/components/examples/GenerateCompletionResponseReproducibleOutputs" }
            WithOptions: { $ref: "#/components/examples/GenerateCompletionResponseWithOptions" }
            LoadModel: { $ref: "#/components/examples/GenerateCompletionResponseLoadModel" }
            UnloadModel: { $ref: "#/components/examples/GenerateCompletionResponseUnloadModel" }

    GenerateEmbeddingsResponse:
      description: OK
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/GenerateEmbeddingsResponse"
          examples:
            SingleInput: { $ref: "#/components/examples/GenerateEmbeddingsResponseSingleInput" }
            MultipleInput: { $ref: "#/components/examples/GenerateEmbeddingsResponseMultipleInput" }

    ListLocalModelsResponse:
      description: OK
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/ListLocalModelsResponse"
          examples:
            Response: { $ref: "#/components/examples/ListLocalModelsResponse" }

    ShowModelInformationResponse:
      description: OK
      content:
        application/json:
          schema:
            $ref: "#/components/schemas/ShowModelInformationResponse"

  schemas:

    CreateModelRequest:
      type: object
      required:
        - name
      properties:
        name:
          type: string
          description: Name of the model to create.
        modelfile:
          type: string
          description: Contents of the `Modelfile`.
        stream:
          $ref: "#/components/schemas/Stream"
        path:
          type: string
          description: Path to the `Modelfile`.

    CreateModelResponse: # TODO
      type: object

    Format:
      type: string
      description: |
        The format to return a response in.
        Format can be `json` or a JSON schema.

    Function:
      type: object
      properties:
        name:
          type: string
        description:
          type: string
        parameters:
          type: object
          properties:
            type:
              type: string
            properties:
              type: object
            required:
              type: array
              items:
                type: string

    GenerateChatCompletionRequest:
      type: object
      required:
        - model
      properties:
        model:
          $ref: "#/components/schemas/ModelName"
        messages:
          type: array
          description: The messages of the chat, this can be used to keep a chat memory.
          items:
            $ref: "#/components/schemas/Message"
        tools:
          $ref: "#/components/schemas/Tools"
        think:
          $ref: "#/components/schemas/Think"
        format:
          $ref: "#/components/schemas/Format"
        options:
          $ref: "#/components/schemas/Options"
        stream:
          $ref: "#/components/schemas/Stream"
        keep_alive:
          $ref: "#/components/schemas/KeepAlive"

    GenerateChatCompletionResponse: # TODO
      type: object
      properties:
        messages:
          type: array
          description: List of messages exchanged in the conversation.
          items:
            $ref: "#/components/schemas/Message"

    GenerateCompletionRequest:
      type: object
      required:
        - model
      properties:
        model:
          $ref: "#/components/schemas/ModelName"
        prompt:
          type: string
          description: The prompt to generate a response for.
          example: Why is the sky blue?
        suffix:
          type: string
          description: The text after the model response.
          example: # TODO
        images:
          $ref: "#/components/schemas/Images"
        think:
          $ref: "#/components/schemas/Think"
        format:
          $ref: "#/components/schemas/Format"
        options:
          $ref: "#/components/schemas/Options"
        system:
          description: System message to (overrides what is defined in the `Modelfile`).
        template:
          description: The prompt template to use (overrides what is defined in the `Modelfile`).
        stream:
          $ref: "#/components/schemas/Stream"
        raw:
          type: boolean
          description: |
            If `true` no formatting will be applied to the prompt.
            You may choose to use the raw parameter if you are specifying
            a full templated prompt in your request to the API.
        keep_alive:
          $ref: "#/components/schemas/KeepAlive"
        context:
          type: array
          deprecated: true
          description: |
            The context parameter returned from a previous request to `/generate`,
            this can be used to keep a short conversational memory.
          items:
            type: number
            format: double

    GenerateCompletionResponse: # TODO
      type: object
      properties:
        text:
          type: string
          description: The generated response text.
          example: "The sky appears blue because of the way Earth's atmosphere scatters light."

    GenerateEmbeddingsRequest:
      type: object
      required:
        - model
      properties:
        model:
          $ref: "#/components/schemas/ModelName"
        input:
          type: string
          description: Text or list of text to generate embeddings for.
        truncate:
          type: boolean
          default: true
          description: |
            Truncates the end of each input to fit within context length.
            Returns error if false and context length is exceeded.
        options:
          $ref: "#/components/schemas/Options"
        keep_alive:
          $ref: "#/components/schemas/KeepAlive"

    GenerateEmbeddingsResponse:
      type: object
      properties:
        model:
          $ref: "#/components/schemas/ModelName"
        embeddings:
          type: array
          items:
            type: array
            items:
              type: number
              format: double

    Images:
      type: array
      description: A list of base64-encoded images (for multimodal models such as `llava`).
      items:
        type: string

    KeepAlive:
      type: integer
      description: Controls how long the model will stay loaded into memory following the request.
      default: 5m

    ListLocalModelsResponse: # TODO
      type: object

    ModelName:
      type: string
      description: |
        Model name, that follow a `model:tag` format, where model can have an optional namespace such as `example/model`.
        Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`.
        The tag is used to identify a specific version.
      example: llama3.2

    Message:
      type: object
      properties:
        role:
          $ref: "#/components/schemas/Role"
        content:
          type: string
          description: The content of the message.
          example: "Why is the sky blue?"
        think:
          $ref: "#/components/schemas/Think"
        images:
          $ref: "#/components/schemas/Images"
        tool_calls: # TODO
          type: array

    Options:
      type: object
      description: |
        Additional model parameters listed in the documentation for the
        [`Modelfile`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)
        such as `temperature`.
      properties:
        mirostat:
          type: integer
        mirostatEta:
          type: number
          format: double
        mirostatTau:
          type: number
          format: double
        repeatLastN:
          type: integer
        temperature:
          type: number
          format: double
        topK:
          type: integer
        topP:
          type: number
          format: double
        repeatPenalty:
          type: number
          format: double
        seed:
          type: integer
        numPredict:
          type: integer
        numCtx:
          type: integer
        stop:
          type: array
          items:
            type: string
        minP:
          type: number
          format: double

    Role:
      type: string
      description: The role of the message.
      enum:
        - system
        - user
        - assistant
        - tool

    ShowModelInformationRequest:
      type: object
      required:
        - model
      properties:
        model:
          $ref: "#/components/schemas/ModelName"
        verbose:
          type: boolean
          description: If set to `true`, returns full data for verbose response fields.

    ShowModelInformationResponse: # TODO
      type: object

    Stream:
      type: boolean
      default: true
      description: |
        If `false` the response will be returned as a single
        response object, rather than a stream of objects.

    Think:
      type: boolean
      description: (for thinking models) should the model think before responding?

    Tools:
      type: array
      description: |
        List of tools in JSON for the model to use if supported.
      items:
        type: object
        properties:
          type:
            type: string
          function:
            $ref: "#/components/schemas/Function"
